{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb6b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Used for storing and playing with data using DataFrame.\n",
    "import pandas as pd\n",
    "#Used for working with arrays in python.\n",
    "import numpy as np\n",
    "#Used for plotting graphs from data.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "!pip install geopandas\n",
    "import plotly.express as px\n",
    "import geopandas as gpd\n",
    "#Used for creating ML models.\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.tree import plot_tree\n",
    "#used for train test splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Distrubation test\n",
    "import scipy\n",
    "from scipy.stats import skew, kurtosis, shapiro, jarque_bera\n",
    "#Normalization \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "# Read csv data\n",
    "data = pd.read_csv('RIA02.20230502T160508.csv')\n",
    "#For Europe map and Anova analysis\n",
    "world_data=pd.read_excel(\"prc_colc_rents_page_spreadsheet.xlsx\")\n",
    "#alpha_code=pd.read_excel(\"Alpha_codes.xlsx\")\n",
    "#Anova Test\n",
    "from scipy.stats import f_oneway\n",
    "#Tukey HDS Test\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f59d10",
   "metadata": {},
   "source": [
    "### Number of Bedrooms\n",
    "One bed :1\n",
    "Two bed : 2\n",
    "Three bed : 3\n",
    "Four plus bed: 4 \n",
    "\n",
    "### Property Type\n",
    "Other flats: 1\n",
    "Apartment: 2\n",
    "Terrace house : 3\n",
    "Semi detached house: 4\n",
    "Detached house : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAta Overview\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a31fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c67fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect null observation number and data type\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130211fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overview for Numeric Variables \n",
    "# The data between 2008 to 2021 \n",
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1270dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Compare \n",
    "#Ireland is among the top 5 countries in terms of per capita income.\n",
    "#That is why I chose to make the comparison between the capitals of these countries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b10c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## World Renting Visulation \n",
    "# it is to understand  both difference and similarity between  Europe Capitals Home Rents and  Dublin  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alpha code to print europe map\n",
    "country_codes = pd.read_csv(\"country_codes.tsv\", sep='\\t')\n",
    "country_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808bf424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check word data\n",
    "world_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b285ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnecessery columns\n",
    "world_data.drop(\"TIME\", axis=1,inplace=True)\n",
    "world_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a99c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#relocation of rows and columns to make them suitable for testing\n",
    "world_data_anova=world_data.transpose()\n",
    "world_data_anova = world_data_anova.rename(columns=world_data_anova.iloc[0]).drop(world_data_anova.index[0])\n",
    "world_data_anova.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc39fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 5 contry ( GDP per capita)\n",
    "filtered_df = world_data_anova[['Denmark','Ireland','Norway', 'Switzerland','Luxembourg']]\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3743b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization TEst \n",
    "#I applied Shapiro Wilk because I have just 20 rows. This test is more reliable on the data that is small\n",
    "for column in filtered_df.columns:\n",
    "    stat, p_value = shapiro(filtered_df[column])\n",
    "    print(f\"City {column}: Sattictic={stat:.4f}, p değeri={p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0915274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Denmark and Luxembourg have normal distribution\n",
    "#IRelan Norway and Switzerland doesnt have normal distrubution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5536e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I used a one-way ANOVA because I compared 5 different countries and the values changed over time.\n",
    "# ANOVA testi for series that have normal distrubation \n",
    "anova_result = f_oneway(world_data_anova['Ireland'], world_data_anova['Luxembourg'], \n",
    "                        world_data_anova['Denmark'], world_data_anova['Switzerland'], \n",
    "                        world_data_anova['Norway'])\n",
    "\n",
    "print(\"ANOVA Test Result:\")\n",
    "print(\"F statisctic:\", anova_result.statistic)\n",
    "print(\"p value :\", anova_result.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1980c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion: p value < 0.05  H0 can not acceptable , at least one series is different from the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "\n",
    "#I used this test because it did not satisfy the normality assumption of some columns. \n",
    "# Kruskal-Wallis testi\n",
    "test_stat, p_value = kruskal(*filtered_df.values.T)\n",
    "\n",
    "# Sonuçları yazdırma\n",
    "print(\"Kruskal-Wallis Test Result :\")\n",
    "print(\"Test statistic:\", test_stat)\n",
    "print(\"p value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201da82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a3275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mann Whitney U\n",
    "from scipy.stats import mannwhitneyu\n",
    "columns = ['Denmark','Ireland','Norway', 'Switzerland','Luxembourg']\n",
    "for i, col in enumerate(columns):\n",
    "    stat, p_value = mannwhitneyu(filtered_df['Ireland'], filtered_df[sütun])\n",
    "    print(f\" {col}: U statistic={stat:.4f}, p value={p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfb863e",
   "metadata": {},
   "source": [
    "### As a result, there is no statistically significant difference between Irelan and Denmark, Ireland, Norway and Switzerland columns. However, it can be said that there is a statistically significant difference between the Luxembourg column and Ireland. These interpretations are based on p-values at the 0.05 significance level.\n",
    "### I deliberately put the data for Ireland in because I wanted to check if the test result would come  as 1. so that I could be sure that the test was reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5794079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the data into the correct format for visualization\n",
    "melted_data = pd.melt(world_data, id_vars='Country', var_name='Year', value_name='Rentt')\n",
    "melted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3797eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge with alpha code\n",
    "merged_data = melted_data.merge(country_codes, left_on='Country', right_on='Country', how='left')\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5e9b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#when we compare the European Capital's Rent Price , Ireland one od the most expensive country in the Europe.\n",
    "# That's why I investigate  Dublin Rent Prices in this assignment\n",
    "\n",
    "fig = px.choropleth(merged_data, \n",
    "                    locations=\"Alpha-3 code\",\n",
    "                    color=\"Rentt\", \n",
    "                    hover_name=\"Country\", \n",
    "                    # column to add to hover information\n",
    "                    animation_frame=\"Year\", \n",
    "                    # column on which to animate\n",
    "                    color_continuous_scale=px.colors.sequential.Plasma )\n",
    "                    \n",
    "fig.update_layout(\n",
    "    # add a title text for the plot\n",
    "    title_text = 'REnt of Europe Capital Cities',\n",
    "    # set projection style for the plot\n",
    "    geo_scope = 'europe' \n",
    "\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07a7a2f",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef37aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows which doesnt have rent price information\n",
    "data.dropna(subset=[\"VALUE\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3b966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop first and fifth row\n",
    "data=data.iloc[:,[1,2,3,4,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7b1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check the code\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a18a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAp  Visualatisiton\n",
    "# data for country visulation \n",
    "country_data = data[(data['Number of Bedrooms'] == 'All bedrooms')& (data[\"Property Type\"]==\"All property types\")\n",
    "                    &data[\"Location\"].isin([\"Dublin\",\"Cork\",\"Limerick\",\"Galway\",\"Waterford\",\"Drogheda, Louth\",\"Dundalk, Louth\"])]\n",
    "# & (df['sütun3'].isin(['Ccc', 'Ddd']))\n",
    "# & (data['Property Type'] == 'All propert types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fcb2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "locat = sorted(country_data['Location'].unique())\n",
    "locat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b85919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Dünya haritası oluşturun\n",
    "world_map = folium.Map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828e4adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sehirler = {'Cork': (51.8985, -8.4756),\n",
    "            'Drogheda, Louth': (53.7189, -6.3561),\n",
    "            'Dublin': (53.3498, -6.2603),\n",
    "            'Dundalk, Louth': (54.0037, -6.4029),\n",
    "            'Galway': (53.2707, -9.0568),\n",
    "            'Limerick': (52.6638, -8.6267),\n",
    "            'Waterford': (52.2593, -7.1101)}\n",
    "\n",
    "# Şehir markörlerini haritaya ekle\n",
    "for sehir, koordinatlar in sehirler.items():\n",
    "    folium.Marker(location=koordinatlar, tooltip=sehir).add_to(world_map)\n",
    "\n",
    "# Haritayı görüntüle\n",
    "world_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a385c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gp\n",
    "ulke = gp.read_file(\"countries.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d91b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ulke.head(10) #ilk 10 satır "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0129134",
   "metadata": {},
   "outputs": [],
   "source": [
    "ireland = ulke[ulke['ADMIN'] == 'Ireland']\n",
    "ireland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45464f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning unneccesary rows because We analize the houses by feature \n",
    "data=data.loc[ (data[\"Number of Bedrooms\"].isin([\"One bed\",\"Two bed\",\"Three bed\",\"Four plus bed\"]))\n",
    "              & (data[\"Property Type\"] != \"All property types\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd6213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get information just about Dublin\n",
    "dub_data=data[data[\"Location\"].str.contains(\"Dublin\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = sorted(dub_data['Location'].unique())\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ea7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this analays based on Dublin areas ( Dublin 22 has any unique information so Dublin 22 Clondalkin is used for Dublin 22 area. \n",
    "#Dublin 22 consist of Clondalkin, Liffey Valley, Newcastle and Neilstown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b738847",
   "metadata": {},
   "outputs": [],
   "source": [
    "dub_data=dub_data.loc[dub_data[\"Location\"].isin([\"Dublin 1\",\"Dublin 2\",\"Dublin 3\",\"Dublin 4\",\n",
    "                           \"Dublin 5\",\"Dublin 6\",\"Dublin 6W\",\"Dublin 7\",\"Dublin 8\",\n",
    "                           \"Dublin 9\",\"Dublin 10\",\"Dublin 11\",\"Dublin 12\",\n",
    "                           \"Dublin 13\",\"Dublin 14\",\"Dublin 15\",\"Dublin 16\"\n",
    "                           ,\"Dublin 17\",\"Dublin 18\",\"Dublin 20\",\"Dublin 22\",\"Dublin 24\",\"Dun Laoghaire, Dublin\"\n",
    "                          ,\"Blackrock, Dublin\",\"Swords, Dublin\",\"Malahide, Dublin\",\"Balbriggan, Dublin\",\"Clondalkin, Dublin 22\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dfc03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dub_data=dub_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To learn any null values and 3324 rows\n",
    "dub_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610b95f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To know how many unique values has each collumns\n",
    "for collumnss in [\"Year\",\"Number of Bedrooms\",\"Property Type\"]:\n",
    "    print(collumnss)\n",
    "    print(dub_data[collumnss].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23241018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To detect any duplicated rows\n",
    "duplicate=dub_data.duplicated()\n",
    "print(\"Count of Dublicated rows :\" ,duplicate.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8197f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to detecet outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b59aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=dub_data[\"VALUE\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e5925",
   "metadata": {},
   "outputs": [],
   "source": [
    "dub_data.boxplot(column=\"VALUE\",by=\"Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outliers\n",
    "# 1st quartile\n",
    "q1 = np.percentile(dub_data[\"VALUE\"],25)\n",
    "# 3rd quartile\n",
    "q3 = np.percentile(dub_data[\"VALUE\"],75)\n",
    "  # IQR\n",
    "IQR = q3 - q1\n",
    "# Outlier step\n",
    "outlier_step = IQR * 1.5\n",
    "# detect outlier and\n",
    "    # store \n",
    "outliers = dub_data[(dub_data[\"VALUE\"] < q1 - outlier_step) | (dub_data[\"VALUE\"] > q3 + outlier_step)]\n",
    "print(outliers.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0178d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distrubution of outliers\n",
    "outliers[\"Location\"].value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d578ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in var :\n",
    "    plot_hist(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#127 rows\n",
    "outliers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb890bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outliar percantege ( outliers/full data) %3 \n",
    "print(127/3324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of outliers by Number of Bedrooms\n",
    "x=dub_data[\"Number of Bedrooms\"].value_counts()\n",
    "y=outliers[\"Number of Bedrooms\"].value_counts()\n",
    "percentage=y/x\n",
    "percentage.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebfad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of outliers by location\n",
    "x=dub_data[\"Location\"].value_counts()\n",
    "y=outliers[\"Location\"].value_counts()\n",
    "percentage=y/x\n",
    "percentage.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41e946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##percentage of outliers by Property Type\n",
    "x=dub_data[\"Property Type\"].value_counts()\n",
    "y=outliers[\"Property Type\"].value_counts()\n",
    "percentage=y/x\n",
    "percentage.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##percentage of outliers by Year\n",
    "x=dub_data[\"Year\"].value_counts()\n",
    "y=outliers[\"Year\"].value_counts()\n",
    "percentage=y/x\n",
    "percentage.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee247b4",
   "metadata": {},
   "source": [
    "### The study will continue with outlier values(%3). As a result of the analysis, it is seen that the outlier values are realized in the location, kind of house and number of rooms, which are expected to have higher values. Also, outlier values have intensified in recent years. It is an expected situation due to inflation. Now The outlier values will not be intervened to avoid information loss. \n",
    "### distribution statistics and visualition tools will be checked to make a final decision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4ee64d",
   "metadata": {},
   "source": [
    "##  Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a76f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String variables is changed with numeric variables . The numerical values will be assigned according to the average value.\n",
    "\n",
    "dub_data.groupby(\"Number of Bedrooms\", as_index=False)['VALUE'].mean().sort_values(by=\"VALUE\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dub_data[\"Number of Bedrooms\"] = dub_data[\"Number of Bedrooms\"].replace({'One bed': 1,\n",
    "                                                                       \"Two bed\": 2,\n",
    "                                                                       \"Three bed\" : 3,\n",
    "                                                                       \"Four plus bed\": 4 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bfb458",
   "metadata": {},
   "outputs": [],
   "source": [
    "dub_data.groupby(\"Property Type\", as_index=False)['VALUE'].mean().sort_values(by=\"VALUE\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90ee718",
   "metadata": {},
   "outputs": [],
   "source": [
    "dub_data[\"Property Type\"] = dub_data[\"Property Type\"].replace({'Other flats': 1,\n",
    "                                                                       \"Apartment\": 2,\n",
    "                                                                       \"Terrace house\" : 3,\n",
    "                                                                       \"Semi detached house\": 4,\n",
    "                                                                        \"Detached house\" : 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69905dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dub_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bbfb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look for data. \n",
    "dub_data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eee41d",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8ef11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this visualisation method used for to see different between the categories also, frequency, min and max value and median\n",
    "# the houses that has 4 or more bedrooms , rent price has big variance . it may have many outliers \n",
    "# other count of bedrooms mod and median values look like close\n",
    "sns.violinplot(data=dub_data, x=\"Number of Bedrooms\", y=\"VALUE\")\n",
    "plt.title(\"Violin Plot of Number of Bedrooms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca99f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apartments  may has some outliers\n",
    "# number 3 or 4 looks pretty close each other as mean,mod and frequency\n",
    "sns.violinplot(data=dub_data, x=\"Property Type\", y=\"VALUE\")\n",
    "plt.title(\"Violin Plot of Property Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8b2ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as both the number of property types and the number of bedrooms increase in value, the value increases\n",
    "sns.scatterplot(\n",
    "    data=dub_data, x=\"Number of Bedrooms\", y=\"Property Type\", hue=\"VALUE\", size=\"VALUE\",\n",
    "    sizes=(20, 200)\n",
    ")\n",
    "plt.title(\"Scatter Plot of Number of Bedrooms and Property Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c65794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number 4 has biggest variance also average rent different each groups\n",
    "g = sns.catplot(\n",
    "    data=dub_data, x=\"Number of Bedrooms\", y=\"VALUE\", \n",
    "    kind=\"bar\", height=4, aspect=.9,\n",
    ")\n",
    "g.set_axis_labels(\"\", \"Rent\")\n",
    "g.despine(left=True)\n",
    "plt.title(\"Bar Plot of Number of Bedrooms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number 5 has biggest variance also average rent different each groups\n",
    "g = sns.catplot(\n",
    "    data=dub_data, x=\"Property Type\", y=\"VALUE\", \n",
    "    kind=\"bar\", height=4, aspect=.9,\n",
    ")\n",
    "g.set_axis_labels(\"\", \"Rent\")\n",
    "g.despine(left=True)\n",
    "plt.title(\"Violin Plot of Property Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c9f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#propert type 4 and 3 have similar properties. therefore they will be combined into a single group\n",
    "dub_data.groupby(\"Property Type\", as_index=False)['VALUE'].agg([\"mean\",\"var\",\"median\"]).sort_values(by=\"mean\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7182f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corelattion before grouping \n",
    "dub_data[\"VALUE\"].corr(dub_data[\"Property Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24b66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dub_data[\"Property Type\"]= [3 if i==3 else 3 if i==4 else i for i in dub_data[\"Property Type\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a81b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(\n",
    "    data=dub_data, x=\"Property Type\", y=\"VALUE\", \n",
    "    kind=\"bar\", height=4, aspect=.9,\n",
    ")\n",
    "g.set_axis_labels(\"\", \"Rent\")\n",
    "g.despine(left=True)\n",
    "plt.title(\"Violin Plot of New Property Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b83ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as number of years increase, the Value  increases (except 2009-2010 \\ 2021)\n",
    "#rents have started to exceed 3000 euros since 2015\n",
    "sns.scatterplot(data=dub_data, x=\"Year\", y=\"VALUE\")\n",
    "plt.title(\"SCatter Plot of VALUE by Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca6e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=dub_data, x=\"Year\", y=\"VALUE\", hue=\"Number of Bedrooms\")\n",
    "plt.title(\"Scatter Plot of VALUE by Year with Number of Beedroms \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40150d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heat map for correlation\n",
    "#number of bedrooms has the highest corellation with VALUE\n",
    "#property type has the lowest corellation with VALUE\n",
    "#after grouping , the corellation increased from 36 to 40\n",
    "sns.heatmap(dub_data.corr(),annot=True,fmt=\".3f\")\n",
    "plt.title(\"Corellation Matrix of Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a8c51d",
   "metadata": {},
   "source": [
    "### Distrubation Analys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dub_data[\"VALUE\"])\n",
    "plt.title(\"Distrubation plot of VALUE \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24e2322",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = skew(dub_data['VALUE'])\n",
    "kurtosis = kurtosis(dub_data['VALUE'])\n",
    "\n",
    "print(\"skewness statistic: \", skewness)\n",
    "print(\"kurtosis statistic: \", kurtosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c7c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kurtosis close 3 . the data has normal distribation (mesokurtic)\n",
    "# skewness 1.4 . the data has positively skewed distribation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b473c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "statis, pvalue = shapiro(dub_data[\"VALUE\"])\n",
    "print('Test statistic: ', statis)\n",
    "print('p-value: ', pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddeb186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This method is used for small data \n",
    "# p-value < 0.05 data set does not follow a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9212ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "jb_statis, pvalue = jarque_bera(dub_data[\"VALUE\"])\n",
    "print(pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f64db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p-value < 0.05 data set does not follow a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization with Min Max Scaler\n",
    "scaler = MinMaxScaler()\n",
    "MM_norm_data= scaler.fit_transform(dub_data[[\"VALUE\"]])\n",
    "MM_norm_data=pd.DataFrame(MM_norm_data,columns=[\"VALUE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb3278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(MM_norm_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dub_data[\"VALUE\"])\n",
    "plt.title(\"Distrubation plot of VALUE After Min Max Scaler \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fe0067",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness1 = skew(MM_norm_data['VALUE'])\n",
    "#kurto_val = kurtosis(MM_norm_data['VALUE'])\n",
    "print(\"skewness statistic: \", skewness1)\n",
    "#print(\"kurtosis statistic: \", kurto_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1574185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skewness 1.4 . the data has positively skewed distribation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15165062",
   "metadata": {},
   "outputs": [],
   "source": [
    "jb_statis, pvalue = jarque_bera(dub_data[\"VALUE\"])\n",
    "print(pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc8d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust transform is more effective on data with outliers\n",
    "scaler = RobustScaler()\n",
    "RBS_data= scaler.fit_transform(dub_data[[\"VALUE\"]])\n",
    "RBS_data=pd.DataFrame(RBS_data,columns=[\"VALUE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c08edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(RBS_data[\"VALUE\"])\n",
    "plt.title(\"Distrubation plot of VALUE after Robust Transform \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b989d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew2 = skew(RBS_data['VALUE'])\n",
    "#kurto2 = kurtosis(RBS_data[\"VALUE\"])\n",
    "print(\"skewness statistic: \", skew2)\n",
    "#print(\"kurtosis statistic: \", kurto2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c51516",
   "metadata": {},
   "outputs": [],
   "source": [
    "jb_statis, pvalue = jarque_bera(RBS_data[\"VALUE\"])\n",
    "print(pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b8417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p-value < 0.05 data set does not follow a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c13dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7532310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dubnew_data = dub_data[(dub_data[\"VALUE\"] >= q1 - outlier_step) & (dub_data[\"VALUE\"] <= q3 + outlier_step)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0288bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 127 datas was removed\n",
    "dubnew_data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dubnew_data[\"VALUE\"])\n",
    "plt.title(\"Distrubation plot of VALUE without outliers \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec3e87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dubnew_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98626e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness_new = skew(dubnew_data['VALUE'])\n",
    "#kurtosis_new = kurtosis(dubnew_data['VALUE'])\n",
    "\n",
    "print(\"skewness statistic: \", skewness_new)\n",
    "#print(\"kurtosis statistic: \", kurtosis_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a8d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skewness 0.6 . the data has symmetrical distribation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e7ce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummy variable\n",
    "dummy_var=pd.get_dummies(dubnew_data[\"Location\"])\n",
    "#concat dummy variable\n",
    "dubdummy_data=pd.concat([dubnew_data,dummy_var],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bfc437",
   "metadata": {},
   "outputs": [],
   "source": [
    "dubdummy_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3845bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization with Min Max Scaler\n",
    "scaler = MinMaxScaler()\n",
    "dubdummy_data[\"Year\"]= scaler.fit_transform(dubdummy_data[[\"Year\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72956070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop Location column\n",
    "dubdummy_data.drop(\"Location\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d5fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRain TEst Split \n",
    "X_train = dubdummy_data.drop(labels = \"VALUE\",axis=1)\n",
    "y_train = dubdummy_data[\"VALUE\"]\n",
    "#test size %20 test data %80 percent train data\n",
    "#random state controls the shuffling process (number 44 is my hometown number in Turkey)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size =0.20, random_state = 44 )\n",
    "print(\"X_train\",len(X_train))\n",
    "print(\"X_test\",len(X_test))\n",
    "print(\"y_train\",len(y_train))\n",
    "print(\"y_test\",len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5da12ac",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84aac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lineer REgression , Decission Tree, Random Forest Regressor are using for anlaysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73393a25",
   "metadata": {},
   "source": [
    "### Lineer Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8768559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x=x_train.iloc[:,1:8].values\n",
    "#y=y_train.VALUE.values.reshape(-1,1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train,y_train)\n",
    "b0_=linreg.intercept_\n",
    "b1_=linreg.coef_\n",
    "print(\"b0_ :\",b0_)\n",
    "print(\"b1_ ;\", b1_)\n",
    "print(\"R_2 score :\", linreg.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70751b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "#X_train_cons=sm.add_constant(X_train)\n",
    "\n",
    "# Note the difference in argument order\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "predictions = model.predict(X_train) # make the predictions by the model\n",
    "\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d01e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variable \"Clondalkin, Dublin 22 \" is statistically insignificant P>|t| > 0.05 . So I dropClondalkin, Dublin 22 column from the dataset\n",
    "x = X_train.drop([\"Clondalkin, Dublin 22\"], axis=1)\n",
    "\n",
    "# Note the difference in argument order\n",
    "model = sm.OLS(y_train, x).fit()\n",
    "predictions = model.predict(x) # make the predictions by the model\n",
    "\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e688b59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variable \"Dublin 24 \" is statistically insignificant P>|t| > 0.05 . So I drop Dublin 24 column from the dataset\n",
    "x = X_train.drop([\"Clondalkin, Dublin 22\",\"Dublin 24\"], axis=1)\n",
    "\n",
    "# Note the difference in argument order\n",
    "model = sm.OLS(y_train, x).fit()\n",
    "predictions = model.predict(x) # make the predictions by the model\n",
    "\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variable \"Dublin 15 \" is statistically insignificant P>|t| > 0.05 . So I drop Dublin 15 column from the dataset\n",
    "x = X_train.drop([\"Clondalkin, Dublin 22\",\"Dublin 24\",\"Dublin 15\"], axis=1)\n",
    "\n",
    "# Note the difference in argument order\n",
    "model = sm.OLS(y_train, x).fit()\n",
    "predictions = model.predict(x) # make the predictions by the model\n",
    "\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a996c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variable \"Dublin 10 \" is statistically insignificant P>|t| > 0.05 . So I drop Dublin 10 column from the dataset\n",
    "x = X_train.drop([\"Clondalkin, Dublin 22\",\"Dublin 24\",\"Dublin 15\",\"Dublin 10\"], axis=1)\n",
    "\n",
    "# Note the difference in argument order\n",
    "model = sm.OLS(y_train, x).fit()\n",
    "predictions = model.predict(x) # make the predictions by the model\n",
    "\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc98176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variable \"Dublin 17 \" is statistically insignificant P>|t| > 0.05 . So I drop Dublin 17 column from the dataset\n",
    "x = X_train.drop([\"Clondalkin, Dublin 22\",\"Dublin 24\",\"Dublin 15\",\"Dublin 10\",\"Dublin 17\"], axis=1)\n",
    "\n",
    "# Note the difference in argument order\n",
    "model = sm.OLS(y_train, x).fit()\n",
    "predictions = model.predict(x) # make the predictions by the model\n",
    "\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068be485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variable \"Swords, Dublin \" is statistically insignificant P>|t| > 0.05 . So I drop Swords, Dublin column from the dataset\n",
    "x = X_train.drop([\"Clondalkin, Dublin 22\",\"Dublin 24\",\"Dublin 15\",\"Dublin 10\",\"Dublin 17\",\"Swords, Dublin\"], axis=1)\n",
    "\n",
    "# Note the difference in argument order\n",
    "model = sm.OLS(y_train, x).fit()\n",
    "predictions = model.predict(x) # make the predictions by the model\n",
    "\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c519b46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All variables is statistically significant.\n",
    "#Also f prob < 0.05 the last model and dependent variable is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a05624",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lineer = X_test.drop([\"Clondalkin, Dublin 22\",\"Dublin 24\",\"Dublin 15\",\"Dublin 10\",\"Dublin 17\",\"Swords, Dublin\"], axis=1)\n",
    "prediction_test=model.predict(X_test_lineer)\n",
    "mse = mean_squared_error(y_test, prediction_test)\n",
    "mae = mean_absolute_error(y_test, prediction_test)\n",
    "R_2 = r2_score(y_test, prediction_test)\n",
    "print(\"mse:\" ,mse)\n",
    "print(\"mae:\", mae)\n",
    "print(\"R_2:\", R_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e392677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#overfitting problem\n",
    "#mse: 44165.21199799048\n",
    "#mae: 170.72865199944485\n",
    "#R_2: 0.7554666834196191"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
